---
title: "HW2"
output: 
  github_document
date: "2023-02-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(modelr)
library(rsample)
library(mosaic)
library(caret)
library(purrr)
library(readr)
library(dplyr)
library(MASS)
library(ROCR)

```

## Question 1

```{r, include = FALSE}
##Data Entry/Train Test Split
set.seed(80)
data(SaratogaHouses)
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)
```


### Improved linear Model 
```{r, echo = FALSE}
##Improved Model wih CV
##CV split

# Define the number of folds
num_folds <- 10

# Create a training control object with repeated cross-validation
train_control <- trainControl(method = "repeatedcv", 
                              number = num_folds, 
                              repeats = 100)

# Train a model using the training control object
model <- train(price ~ . - bathrooms - pctCollege - fireplaces + newConstruction*age + rooms*bedrooms + fireplaces*bedrooms + lotSize*landValue, data = saratoga_train, 
               method = "lm", 
               trControl = train_control)
modelPred = predict(model, saratoga_test)



# View the average RMSE over all folds
mean(RMSE(modelPred, saratoga_test$price))

```



```{r, include = FALSE}
##Medium Model
lm_medium = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse(lm_medium, saratoga_test)
```


```{r, include = FALSE}
##See Above, section obselete
## Improved Model
lm_improved = lm(price ~ . - bathrooms - pctCollege - fireplaces + newConstruction*age + rooms*bedrooms + fireplaces*bedrooms + lotSize*landValue, data = saratoga_test)
rmse(lm_improved, saratoga_test)
```


```{r, include = FALSE}
##Standardizing variables
##changing newConstruction to numeric
saratoga_train$newConstruction = as.numeric(saratoga_train$newConstruction)

saratoga_test$newConstruction = as.numeric(saratoga_test$newConstruction)

##Standardizing training


strain_stand <- 
  saratoga_train %>% 
  mutate(bath_s = scale(bathrooms), pctCollege_s = scale(pctCollege), fireplaces_s= scale(fireplaces), newConstruction_s = scale(newConstruction), age_s = scale(age),rooms_s = scale(rooms), bedrooms_s = scale(bedrooms), lotSize_s = scale(lotSize),landValue_s = scale(landValue))

##StandardizingTesting

stest_stand <- 
  saratoga_test %>% 
  mutate(bath_s = scale(bathrooms), pctCollege_s = scale(pctCollege), fireplaces_s= scale(fireplaces), newConstruction_s = scale(newConstruction), age_s = scale(age),rooms_s = scale(rooms), bedrooms_s = scale(bedrooms), lotSize_s = scale(lotSize),landValue_s = scale(landValue))

```


### KNN RMSE

```{r, echo = FALSE}
##model for KNN
##Model Attempt

model2 <- train(price ~ bath_s + pctCollege_s + fireplaces_s + newConstruction_s + age_s + rooms_s + bedrooms_s +lotSize_s +landValue_s, data = strain_stand,
               method = "kknn", 
               trControl = train_control)

predictions <- predict(model2, stest_stand)
mean(RMSE(predictions, stest_stand$price))
```

```{r, include = FALSE}

##This is not correct - does not include CV

set.seed(123)
## Attempt for loop
k_vec= c(2:346)
knnk = list()
output = list()
rmse_vec = c()
## for loop trim = 350
for (i in 2:length(k_vec)){
  knnk[[i]] = knnreg(price~bath_s + pctCollege_s + fireplaces_s + newConstruction_s + age_s + rooms_s + bedrooms_s +lotSize_s +landValue_s, data = strain_stand, k =  k_vec[i])
output[i] = stest_stand %>%
  mutate(Price_Pred = predict(knnk[[i]], stest_stand))
 ##sclass350_test = sclass350_test %>%
 ## mutate(Price_pred = predict(knnk, sclass350_test))
rmse_vec[i] = modelr::rmse(knnk[[i]], stest_stand)
}
k_vec
rmse_vec
plot(k_vec, rmse_vec)
output
my_data = data.frame(k_vec, rmse_vec)
my_data

my_data = na.omit(my_data)
min(my_data$rmse_vec)
which.min(my_data$rmse_vec)
my_data

```

The average RMSE for the linear model was lower in this case than the average RMSE for the KNN model.  However, since the KNN model was scaled, it might not make sense to interpret these in comparison.  For the taxing authority, which model to use is dependent on the taxing system they want to use.  Using the standardized model, we interpret our predictions in terms of how many standard deviations they are changing the price of the house.  If the taxing authority want to use more of a progressive tax, this model may be better because it is measured in terms of how far a house is from the mean (the number of standard deviations).  However, if the housing authority wants a more accurate prediction, it may make sense to use the linear regression model as it is slightly more accurate and can be interpreted in terms of price.  





## Question 2

```{r, include = FALSE}
##2
##data wrangling
ger = read_csv("Data/german_credit.csv")
library(dplyr)
default_probabilities <- ger %>%
  group_by(history) %>%
  summarise(default_prob = mean(Default))



```

```{r, echo = FALSE}

##2 Bar Graph
library(ggplot2)
ggplot(default_probabilities, aes(x = history, y = default_prob, fill = history)) +
  geom_bar(stat = "identity") +
  xlab("Credit History") +
  ylab("Default Probability") +
  ggtitle("Default Probability by Credit History")


```

```{r, echo = FALSE}
##2 model

m1 = glm(Default ~ duration + amount + installment + age + history + purpose + foreign,
             data = ger,
             family = binomial())
summary(m1)

```
This model is showing that the worse that someone's credit score, the less probability they have of defaulting on a loan.  However, this is not the most accurate prediction given the data we are working with.  Becuase defaults are extremely oversampled in this dataset, when the surveyors attempted to match cases, they looked at people with low and terrible credit scores who did not default.  Therefore, as they included more of these individuals in the study, the probability of default relative to credit score decreased.  Similarly, it is likely that very few people with a good credit score defaulted on loans, and so the ones included in the survey make it seem like a higher probability than it actually was. As such, this is not a good predictive model, and I would suggest to make a predictive model the bank randomly samples all people taking out loans. 

## Question 3

```{r}
hotels_dev = read_csv("Data/hotels_dev.csv")
hotels_val = read.csv("Data/hotels_val.csv")

hotels_dev_split = initial_split(hotels_dev, prop = 0.8)
hotels_dev_train = training(hotels_dev_split)
hotels_dev_test = testing(hotels_dev_split)

m1_hotels_dev = lm(children ~ market_segment + adults + customer_type + is_repeated_guest, data = hotels_dev_train)
m2_hotels_dev = lm(children ~ . - arrival_date, data = hotels_dev_train)
#For our model we used a stepwise selection model using AIC
m3_hotels_dev = stepAIC(m2_hotels_dev, direction = "both", 
              trace = FALSE, k = log(36000))
```


##RMSE Test
```{r, RMSE Test}
rmse(m1_hotels_dev, data = hotels_dev_test)
rmse(m2_hotels_dev, data = hotels_dev_test)
rmse(m3_hotels_dev, data = hotels_dev_test)

# While our RMSE test did not show significan improvement, the stability.simplicity of the resulting model is a welcomed improvement.
```
##Out-of-Sample Predict test
```{r, Out-of-Sample Predict test}
phat_test_children1 = predict(m1_hotels_dev, hotels_dev_test, type = 'response')
yhat_test_children1 = ifelse(phat_test_children1 > 0.5, 1, 0)
confusion_m1 = table(y = hotels_dev_test$children, yhat = yhat_test_children1)
confusion_m1

phat_test_children2 = predict(m2_hotels_dev, hotels_dev_test, type = 'response')
yhat_test_children2 = ifelse(phat_test_children2 > 0.5, 1, 0)
confusion_m2 = table(y = hotels_dev_test$children, yhat = yhat_test_children2)
confusion_m2

phat_test_children3 = predict(m3_hotels_dev, hotels_dev_test)
yhat_test_children3 = ifelse(phat_test_children3 > 0.5, 1, 0)
confusion_m3 = table(y = hotels_dev_test$children, yhat = yhat_test_children3)
confusion_m3

table(hotels_dev_test$children)
```

```{r Accuracy Calculations}

m1_accuracy = sum(diag(confusion_m1))/sum(confusion_m1)
m2_accuracy = sum(diag(confusion_m2))/sum(confusion_m2)
m3_accuracy = sum(diag(confusion_m3))/sum(confusion_m3)
null_accuracy = 8276/(8276+724)

m1_accuracy
m2_accuracy
m3_accuracy
null_accuracy

# Absolute Improvement 

m1_absimpr = m1_accuracy - null_accuracy
m2_absimpr = m2_accuracy - null_accuracy
m3_absimpr = m3_accuracy - null_accuracy

m1_absimpr
m2_absimpr
m3_absimpr

# Lift

m1_lift = m1_accuracy/null_accuracy
m2_lift = m2_accuracy/null_accuracy
m3_lift = m3_accuracy/null_accuracy

m1_lift
m2_lift
m3_lift
```


## Model Validation 

#Step 1

#ROC Curve

```{r}
m3_pred_values <- predict(m3_hotels_dev, hotels_val, type = "response")
m3_pred_data <- prediction(m3_pred_values, hotels_val$children)

roc_data <- performance(m3_pred_data, measure="tpr", x.measure="fpr")

plot(roc_data, main = "ROC Curve for Model 3")

```


# Step 2 - creating K-fold

```{r Step 2 K-fold}
k_folds = 20

hotels_val = hotels_val %>%
  mutate(fold_number = rep(1:k_folds, length = nrow(hotels_val)) %>% sample())

actual_children <- list()
expected_children <- list()
difference_children <- list()

for (x in 1:20) {
  fold <- hotels_val %>% 
    filter(fold_number == x)

phat <- predict(m3_hotels_dev, fold)

expected_children[[x]] <- round(sum(phat), 2)
actual_children [[x]] <- sum(fold$children)
difference_children[[x]] <- round(expected_children[[x]] - actual_children[[x]], 2)
}

fold_id = list(seq(1, 20, by=1))

predict_table = tibble("FOLD_ID" = unlist(fold_id), "EXPECTED" = unlist(expected_children), "ACTUAL" = unlist(actual_children), "DIFFERENCE" = unlist(difference_children))

predict_table
```
# Our model appears to be failry consistent, when looking at the final difference between predictions and actual results. While overall the predicted results are probably acceptable, there are constant outliers after mulitple runs of the model.

