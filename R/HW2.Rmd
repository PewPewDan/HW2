---
title: "HW2"
output: 
  github_document
date: "2023-02-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(tidyverse)
library(tidyverse)
library(ggplot2)
library(modelr)
library(rsample)
library(mosaic)
library(caret)
library(purrr)

```

## Question 1

```{r, include = FALSE}
##Data Entry/Train Test Split
set.seed(80)
data(SaratogaHouses)
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

```


### Improved linear Model 
```{r, echo = FALSE}
##Improved Model wih CV
##CV split

# Define the number of folds
num_folds <- 10

# Create a training control object with repeated cross-validation
train_control <- trainControl(method = "repeatedcv", 
                              number = num_folds, 
                              repeats = 100)

# Train a model using the training control object
model <- train(price ~ . - bathrooms - pctCollege - fireplaces + newConstruction*age + rooms*bedrooms + fireplaces*bedrooms + lotSize*landValue, data = saratoga_train, 
               method = "lm", 
               trControl = train_control)
modelPred = predict(model, saratoga_test)



# View the average RMSE over all folds
mean(RMSE(modelPred, saratoga_test$price))




```



```{r, include = FALSE}
##Medium Model
lm_medium = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmse(lm_medium, saratoga_test)
```


```{r, include = FALSE}
##See Above, section obselete
## Improved Model
lm_improved = lm(price ~ . - bathrooms - pctCollege - fireplaces + newConstruction*age + rooms*bedrooms + fireplaces*bedrooms + lotSize*landValue, data = saratoga_test)
rmse(lm_improved, saratoga_test)
```


```{r, include = FALSE}
##Standardizing variables
##changing newConstruction to numeric
saratoga_train$newConstruction = as.numeric(saratoga_train$newConstruction)

saratoga_test$newConstruction = as.numeric(saratoga_test$newConstruction)

##Standardizing training


strain_stand <- 
  saratoga_train %>% 
  mutate(bath_s = scale(bathrooms), pctCollege_s = scale(pctCollege), fireplaces_s= scale(fireplaces), newConstruction_s = scale(newConstruction), age_s = scale(age),rooms_s = scale(rooms), bedrooms_s = scale(bedrooms), lotSize_s = scale(lotSize),landValue_s = scale(landValue))

##StandardizingTesting

stest_stand <- 
  saratoga_test %>% 
  mutate(bath_s = scale(bathrooms), pctCollege_s = scale(pctCollege), fireplaces_s= scale(fireplaces), newConstruction_s = scale(newConstruction), age_s = scale(age),rooms_s = scale(rooms), bedrooms_s = scale(bedrooms), lotSize_s = scale(lotSize),landValue_s = scale(landValue))

```


### KNN RMSE

```{r, echo = FALSE}
##model for KNN
##Model Attempt

model2 <- train(price ~ bath_s + pctCollege_s + fireplaces_s + newConstruction_s + age_s + rooms_s + bedrooms_s +lotSize_s +landValue_s, data = strain_stand,
               method = "kknn", 
               trControl = train_control)

predictions <- predict(model2, stest_stand)
mean(RMSE(predictions, stest_stand$price))
```

```{r, include = FALSE}

##This is not correct - does not include CV

set.seed(123)
## Attempt for loop
k_vec= c(2:346)
knnk = list()
output = list()
rmse_vec = c()
## for loop trim = 350
for (i in 2:length(k_vec)){
  knnk[[i]] = knnreg(price~bath_s + pctCollege_s + fireplaces_s + newConstruction_s + age_s + rooms_s + bedrooms_s +lotSize_s +landValue_s, data = strain_stand, k =  k_vec[i])
output[i] = stest_stand %>%
  mutate(Price_Pred = predict(knnk[[i]], stest_stand))
 ##sclass350_test = sclass350_test %>%
 ## mutate(Price_pred = predict(knnk, sclass350_test))
rmse_vec[i] = modelr::rmse(knnk[[i]], stest_stand)
}
k_vec
rmse_vec
plot(k_vec, rmse_vec)
output
my_data = data.frame(k_vec, rmse_vec)
my_data

my_data = na.omit(my_data)
min(my_data$rmse_vec)
which.min(my_data$rmse_vec)
my_data

```

The average RMSE for the linear model was lower in this case than the average RMSE for the KNN model.  However, since the KNN model was scaled, it might not make sense to interpret these in comparison.  For the taxing authority, which model to use is dependent on the taxing system they want to use.  Using the standardized model, we interpret our predictions in terms of how many standard deviations they are changing the price of the house.  If the taxing authority want to use more of a progressive tax, this model may be better because it is measured in terms of how far a house is from the mean (the number of standard deviations).  However, if the housing authority wants a more accurate prediction, it may make sense to use the linear regression model as it is slightly more accurate and can be interpreted in terms of price.  





## Question 2

```{r, include = FALSE}
##2
##data wrangling
ger = read_csv("german_credit.csv")
library(dplyr)
default_probabilities <- ger %>%
  group_by(history) %>%
  summarise(default_prob = mean(Default))



```

```{r, echo = FALSE}

##2 Bar Graph
library(ggplot2)
ggplot(default_probabilities, aes(x = history, y = default_prob, fill = history)) +
  geom_bar(stat = "identity") +
  xlab("Credit History") +
  ylab("Default Probability") +
  ggtitle("Default Probability by Credit History")


```

```{r, echo = FALSE}
##2 model

m1 = glm(Default ~ duration + amount + installment + age + history + purpose + foreign,
             data = ger,
             family = binomial())
summary(m1)

```
This model is showing that the worse that someone's credit score, the less probability they have of defaulting on a loan.  However, this is not the most accurate prediction given the data we are working with.  Becuase defaults are extremely oversampled in this dataset, when the surveyors attempted to match cases, they looked at people with low and terrible credit scores who did not default.  Therefore, as they included more of these individuals in the study, the probability of default relative to credit score decreased.  Similarly, it is likely that very few people with a good credit score defaulted on loans, and so the ones included in the survey make it seem like a higher probability than it actually was. As such, this is not a good predictive model, and I would suggest to make a predictive model the bank randomly samples all people taking out loans. 

## Question 3





